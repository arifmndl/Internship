{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb6e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e31e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
      "     ---------------------------------------- 9.4/9.4 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "     -------------------------------------- 400.2/400.2 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.1.3 h11-0.14.0 outcome-1.2.0 selenium-4.12.0 trio-0.22.2 trio-websocket-0.10.4 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc03e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Fill search form and submit\n",
    "search_params = {\n",
    "    \"q\": \"Data Analyst\",\n",
    "    \"l\": \"Bangalore\",\n",
    "}\n",
    "\n",
    "search_url = \"https://www.shine.com/jobsearch/simple/data-analyst/bangalore/\"\n",
    "search_response = requests.get(search_url, params=search_params)\n",
    "search_soup = BeautifulSoup(search_response.content, \"html.parser\")\n",
    "\n",
    "# Step 4: Scrape data for the first 10 jobs\n",
    "jobs = []\n",
    "job_results = search_soup.find_all(\"li\", class_=\"search_listing\")\n",
    "for job_result in job_results[:10]:  # First 10 jobs\n",
    "    title = job_result.find(\"h3\", class_=\"job_title\").text.strip()\n",
    "    company_name = job_result.find(\"p\", class_=\"company_name\").text.strip()\n",
    "    experience_required = job_result.find(\"span\", class_=\"exp\").text.strip()\n",
    "    job_location = job_result.find(\"span\", class_=\"loc\").text.strip()\n",
    "\n",
    "    jobs.append({\n",
    "        \"Job Title\": title,\n",
    "        \"Company Name\": company_name,\n",
    "        \"Experience Required\": experience_required,\n",
    "        \"Job Location\": job_location,\n",
    "    })\n",
    "\n",
    "# Step 5: Create a dataframe\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51320a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Fill search form and submit\n",
    "search_params = {\n",
    "    \"q\": \"Data Scientist\",\n",
    "    \"l\": \"Bangalore\",\n",
    "}\n",
    "\n",
    "search_url = \"https://www.shine.com/jobsearch/simple/data-scientist/bangalore/\"\n",
    "search_response = requests.get(search_url, params=search_params)\n",
    "search_soup = BeautifulSoup(search_response.content, \"html.parser\")\n",
    "\n",
    "# Step 4: Scrape data for the first 10 jobs\n",
    "jobs = []\n",
    "job_results = search_soup.find_all(\"li\", class_=\"search_listing\")\n",
    "for job_result in job_results[:10]:  # First 10 jobs\n",
    "    title = job_result.find(\"h3\", class_=\"job_title\").text.strip()\n",
    "    company_name = job_result.find(\"p\", class_=\"company_name\").text.strip()\n",
    "    job_location = job_result.find(\"span\", class_=\"loc\").text.strip()\n",
    "\n",
    "    jobs.append({\n",
    "        \"Job Title\": title,\n",
    "        \"Company Name\": company_name,\n",
    "        \"Job Location\": job_location,\n",
    "    })\n",
    "\n",
    "# Step 5: Create a dataframe\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea866e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10676\\684303463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=\"path_to_your_chromedriver\")\n",
    "\n",
    "# Step 1: Navigate to the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Fill search form and submit\n",
    "search_input = driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_input.send_keys(\"Data Scientist\")\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Step 3: Wait for results to load\n",
    "driver.implicitly_wait(10)  # Wait for 10 seconds\n",
    "\n",
    "# Step 4: Apply filters\n",
    "location_filter = driver.find_element_by_xpath(\"//label[text()='Location']\")\n",
    "salary_filter = driver.find_element_by_xpath(\"//label[text()='Salary']\")\n",
    "\n",
    "location_filter.click()\n",
    "salary_filter.click()\n",
    "\n",
    "# Step 5: Wait for filters to apply\n",
    "time.sleep(5)  # Wait for 5 seconds\n",
    "\n",
    "# Step 6: Scrape data for the first 10 jobs\n",
    "jobs = []\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "job_results = soup.find_all(\"li\", class_=\"search_listing\")[:10]\n",
    "for job_result in job_results:\n",
    "    title = job_result.find(\"h3\", class_=\"job_title\").text.strip()\n",
    "    company_name = job_result.find(\"p\", class_=\"company_name\").text.strip()\n",
    "    experience_required = job_result.find(\"span\", class_=\"exp\").text.strip()\n",
    "    job_location = job_result.find(\"span\", class_=\"loc\").text.strip()\n",
    "\n",
    "    jobs.append({\n",
    "        \"Job Title\": title,\n",
    "        \"Company Name\": company_name,\n",
    "        \"Experience Required\": experience_required,\n",
    "        \"Job Location\": job_location,\n",
    "    })\n",
    "\n",
    "# Step 7: Create a dataframe\n",
    "df = pd.DataFrame(jobs)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ada8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunglasses Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Sneakers Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape sunglasses data\n",
    "def scrape_sunglasses_data():\n",
    "    sunglasses_data = []\n",
    "    \n",
    "    for page in range(1, 11):  # Scraping from 10 pages (10 x 10 = 100 items)\n",
    "        url = f\"https://www.flipkart.com/search?q=sunglasses&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        products = soup.find_all(\"div\", class_=\"_2kHMtA\")\n",
    "        for product in products:\n",
    "            brand = product.find(\"div\", class_=\"_2WkVRV\").text\n",
    "            description = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "            price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "            \n",
    "            sunglasses_data.append({\n",
    "                \"Brand\": brand,\n",
    "                \"ProductDescription\": description,\n",
    "                \"Price\": price,\n",
    "            })\n",
    "    \n",
    "    return sunglasses_data\n",
    "\n",
    "# Function to scrape sneakers data\n",
    "def scrape_sneakers_data():\n",
    "    sneakers_data = []\n",
    "    \n",
    "    for page in range(1, 11):  # Scraping from 10 pages (10 x 10 = 100 items)\n",
    "        url = f\"https://www.flipkart.com/search?q=sneakers&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        products = soup.find_all(\"div\", class_=\"_2kHMtA\")\n",
    "        for product in products:\n",
    "            brand = product.find(\"div\", class_=\"_2WkVRV\").text\n",
    "            description = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "            price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "            \n",
    "            sneakers_data.append({\n",
    "                \"Brand\": brand,\n",
    "                \"ProductDescription\": description,\n",
    "                \"Price\": price,\n",
    "            })\n",
    "    \n",
    "    return sneakers_data\n",
    "\n",
    "# Scrape sunglasses data\n",
    "sunglasses_data = scrape_sunglasses_data()\n",
    "sunglasses_df = pd.DataFrame(sunglasses_data)\n",
    "print(\"Sunglasses Data:\")\n",
    "print(sunglasses_df)\n",
    "\n",
    "# Scrape sneakers data\n",
    "sneakers_data = scrape_sneakers_data()\n",
    "sneakers_df = pd.DataFrame(sneakers_data)\n",
    "print(\"\\nSneakers Data:\")\n",
    "print(sneakers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29161f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_reviews_data(url, num_reviews):\n",
    "    reviews_data = []\n",
    "    \n",
    "    num_pages = (num_reviews + 9) // 10  # Calculate number of pages to scrape\n",
    "    \n",
    "    for page in range(1, num_pages + 1):\n",
    "        page_url = f\"{url}&page={page}\"\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        reviews = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "        for review in reviews:\n",
    "            rating = review.find(\"div\", class_=\"_3LWZlK\").text\n",
    "            review_summary = review.find(\"p\", class_=\"_2-N8zT\").text\n",
    "            full_review = review.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "            \n",
    "            reviews_data.append({\n",
    "                \"Rating\": rating,\n",
    "                \"Review Summary\": review_summary,\n",
    "                \"Full Review\": full_review,\n",
    "            })\n",
    "            \n",
    "            if len(reviews_data) >= num_reviews:\n",
    "                return reviews_data\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "num_reviews_to_scrape = 100\n",
    "\n",
    "reviews_data = scrape_reviews_data(url, num_reviews_to_scrape)\n",
    "reviews_df = pd.DataFrame(reviews_data)\n",
    "print(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c190b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_sneakers_data(num_sneakers):\n",
    "    sneakers_data = []\n",
    "    \n",
    "    url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    products = soup.find_all(\"div\", class_=\"_2kHMtA\")\n",
    "    for product in products[:num_sneakers]:\n",
    "        brand = product.find(\"div\", class_=\"_2WkVRV\").text\n",
    "        description = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "        price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "        \n",
    "        sneakers_data.append({\n",
    "            \"Brand\": brand,\n",
    "            \"ProductDescription\": description,\n",
    "            \"Price\": price,\n",
    "        })\n",
    "    \n",
    "    return sneakers_data\n",
    "\n",
    "num_sneakers_to_scrape = 100\n",
    "\n",
    "sneakers_data = scrape_sneakers_data(num_sneakers_to_scrape)\n",
    "sneakers_df = pd.DataFrame(sneakers_data)\n",
    "print(sneakers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a3ccad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10676\\1293018121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=\"path_to_your_chromedriver\")\n",
    "\n",
    "# Step 1: Navigate to the webpage\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Enter \"Laptop\" in the search field and click the search icon\n",
    "search_input = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_input.send_keys(\"Laptop\")\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Step 3: Apply CPU Type filter \"Intel Core i7\"\n",
    "cpu_filter = driver.find_element(By.XPATH, \"//li[@aria-label='Intel Core i7']\")\n",
    "cpu_filter.click()\n",
    "\n",
    "# Step 4: Wait for filter to be applied\n",
    "time.sleep(2)  # Wait for 2 seconds\n",
    "\n",
    "# Step 5: Scrape data for the first 10 laptops\n",
    "laptops_data = []\n",
    "laptops = driver.find_elements(By.CSS_SELECTOR, \".s-result-item.s-asin.sg-col-0-of-12.sg-col-16-of-20.sg-col.sg-col-12-of-16\")\n",
    "for laptop in laptops[:10]:  # First 10 laptops\n",
    "    title = laptop.find_element(By.CSS_SELECTOR, \".a-size-medium.a-color-base.a-text-normal\").text\n",
    "    \n",
    "    try:\n",
    "        ratings = laptop.find_element(By.CSS_SELECTOR, \".a-icon-alt\").get_attribute(\"innerHTML\")\n",
    "    except:\n",
    "        ratings = \"Not available\"\n",
    "    \n",
    "    try:\n",
    "        price = laptop.find_element(By.CSS_SELECTOR, \".a-price-whole\").text\n",
    "    except:\n",
    "        price = \"Not available\"\n",
    "    \n",
    "    laptops_data.append({\n",
    "        \"Title\": title,\n",
    "        \"Ratings\": ratings,\n",
    "        \"Price\": price\n",
    "    })\n",
    "\n",
    "# Step 6: Create a dataframe\n",
    "df = pd.DataFrame(laptops_data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e99580f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10676\\1008640770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=\"path_to_your_chromedriver\")\n",
    "\n",
    "# Step 1: Navigate to the webpage\n",
    "url = \"https://www.azquotes.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Click on \"Top Quotes\"\n",
    "top_quotes_link = driver.find_element(By.PARTIAL_LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Step 3: Wait for the quotes to load\n",
    "time.sleep(5)  # Wait for 5 seconds\n",
    "\n",
    "# Step 4: Scrape data for the top 1000 quotes\n",
    "quotes_data = []\n",
    "quotes_elements = driver.find_elements(By.CSS_SELECTOR, \".title a\")\n",
    "for quote_element in quotes_elements:\n",
    "    quote = quote_element.find_element(By.CLASS_NAME, \"title\").text\n",
    "    author = quote_element.find_element(By.CLASS_NAME, \"author\").text\n",
    "    type_of_quotes = quote_element.find_element(By.CLASS_NAME, \"author\").find_element(By.TAG_NAME, \"small\").text\n",
    "    \n",
    "    quotes_data.append({\n",
    "        \"Quote\": quote,\n",
    "        \"Author\": author,\n",
    "        \"Type Of Quotes\": type_of_quotes,\n",
    "    })\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(quotes_data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1dcb97e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10676\\673571942.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Set up Chrome WebDriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=\"path_to_your_chromedriver\")\n",
    "\n",
    "# Step 1: Navigate to the webpage\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Click on \"GK\" option\n",
    "gk_option = driver.find_element(By.LINK_TEXT, \"GK\")\n",
    "gk_option.click()\n",
    "\n",
    "# Step 3: Click on \"List of all Prime Ministers of India\"\n",
    "pm_list_option = driver.find_element(By.LINK_TEXT, \"List of all Prime Ministers of India\")\n",
    "pm_list_option.click()\n",
    "\n",
    "# Step 4: Scrape data for respected former Prime Ministers\n",
    "pm_data = []\n",
    "rows = driver.find_elements(By.XPATH, \"//table[@class='tableStyle']//tr\")[1:]  # Skip header row\n",
    "for row in rows:\n",
    "    columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    name = columns[0].text\n",
    "    born_dead = columns[1].text\n",
    "    term_of_office = columns[2].text\n",
    "    remarks = columns[3].text\n",
    "    \n",
    "    pm_data.append({\n",
    "        \"Name\": name,\n",
    "        \"Born-Dead\": born_dead,\n",
    "        \"Term of Office\": term_of_office,\n",
    "        \"Remarks\": remarks,\n",
    "    })\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(pm_data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4829505",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10676\\3408207156.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Set up Chrome WebDriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=\"path_to_your_chromedriver\")\n",
    "\n",
    "# Step 1: Navigate to the webpage\n",
    "url = \"https://www.motor1.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Type in the search bar '50 most expensive cars' and click the search button\n",
    "search_bar = driver.find_element(By.NAME, \"q\")\n",
    "search_bar.send_keys(\"50 most expensive cars\")\n",
    "search_bar.submit()\n",
    "\n",
    "# Step 3: Click on \"50 most expensive cars in the world...\"\n",
    "expensive_cars_link = driver.find_element(By.PARTIAL_LINK_TEXT, \"50 Most Expensive Cars In The World\")\n",
    "expensive_cars_link.click()\n",
    "\n",
    "# Step 4: Scrape data for the 50 most expensive cars\n",
    "cars_data = []\n",
    "cars = driver.find_elements(By.CSS_SELECTOR, \".vehicle\")\n",
    "for car in cars:\n",
    "    car_name = car.find_element(By.CLASS_NAME, \"vehicle-title\").text\n",
    "    car_price = car.find_element(By.CLASS_NAME, \"vehicle-price\").text\n",
    "    \n",
    "    cars_data.append({\n",
    "        \"Car Name\": car_name,\n",
    "        \"Price\": car_price,\n",
    "    })\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(cars_data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e17d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
